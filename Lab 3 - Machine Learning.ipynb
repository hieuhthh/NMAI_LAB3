{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GUvdvAaASzmD"
      },
      "source": [
        "# Lab 3: Machine Learning\n",
        "\n",
        "- MSSV: 20120081\n",
        "- Họ và tên: Nguyễn Mậu Trọng Hiếu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Esno5cNSzmX"
      },
      "source": [
        "## Yêu cầu bài tập\n",
        "\n",
        "**Cách làm bài**\n",
        "\n",
        "\n",
        "Bạn sẽ làm trực tiếp trên file notebook này; trong file, từ `TODO` để cho biết những phần mà bạn cần phải làm.\n",
        "\n",
        "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
        "\n",
        "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
        "\n",
        "**Cách nộp bài**\n",
        "\n",
        "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau đó, tạo thư mục được đặt tên theo MSSV của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`). Copy file `Lab 3 - Machine Learning.ipynb` vào, rồi nén thư mục này lại theo định dạng `.zip` (không nén với các định dạng khác) và nộp ở link trên moodle.\n",
        "\n",
        "**Nội dung bài tập**\n",
        "\n",
        "Bài tập 3 là bài tập cá nhân. Trong bài này, bạn sẽ cài đặt 2 thuật toán học máy: \n",
        "1. Cây quyết định (Decision tree)\n",
        "2. Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYefhgwoSzmZ"
      },
      "source": [
        "### Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uXTmvF6JSzmb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO6JKObwSzmn"
      },
      "source": [
        "### Load Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "6EFp9Jl3Szmo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "iris=datasets.load_iris()\n",
        "\n",
        "X=iris.data\n",
        "y=iris.target\n",
        "\n",
        "# split dataset into training data and testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)\n",
        "\n",
        "# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.9, random_state=68)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-C6eu2Szmt"
      },
      "source": [
        "## 1. Cây quyết định: Iterative Dichotomiser 3 (ID3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEQ7wvnpSzm3"
      },
      "source": [
        "### 1.1 Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9doZ5CKSSzm5"
      },
      "source": [
        "Thông tin kỳ vọng (entropy):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJNW8LxSzm6"
      },
      "source": [
        "$$Entropy=-\\sum_{i}^{n}p_ilog_{2}(p_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-H-41plSzm7"
      },
      "source": [
        "Hàm entropy đạt giá trị nhỏ nhất nếu có một giá trị $p_i=1$, đạt giá trị lớn nhất nếu tất cả các $p_i$ bằng nhau. Những tính chất này của hàm entropy khiến nó được sử dụng trong việc đo độ hỗn loạn của một phép phân chia của ID3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "HGAwlg1dSzm9"
      },
      "outputs": [],
      "source": [
        "def entropy(counts, n_samples):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    counts: shape (n_classes): list number of samples in each class\n",
        "    n_samples: number of data samples\n",
        "    \n",
        "    -----------\n",
        "    return entropy \n",
        "    \"\"\"\n",
        "    # TODO: calculate entropy and return its value\n",
        "    \n",
        "    p_list = np.array(counts) / n_samples\n",
        "\n",
        "    return - np.sum(p_list * np.log2(p_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "EJ7on8pvSznN"
      },
      "outputs": [],
      "source": [
        "def entropy_of_one_division(division): \n",
        "    \"\"\"\n",
        "    Returns entropy of a divided group of data\n",
        "    Data may have multiple classes\n",
        "    \"\"\"\n",
        "    n_samples = len(division)\n",
        "    n_classes = set(division)\n",
        "    \n",
        "    # count samples in each class then store it to list counts\n",
        "    counts=[]\n",
        "    # TODO:\n",
        "\n",
        "    list_division = list(division)\n",
        "\n",
        "    for c in n_classes:\n",
        "        counts.append(list_division.count(c))\n",
        "\n",
        "    return entropy(counts,n_samples),n_samples\n",
        "\n",
        "\n",
        "def get_entropy(y_predict, y):\n",
        "    \"\"\"\n",
        "    Returns entropy of a split\n",
        "    y_predict is the split decision by cutoff, True/Fasle\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    entropy_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
        "    entropy_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
        "    # overall entropy\n",
        "\n",
        "    s = None \n",
        "    # TODO: calculate overall entropy s=?\n",
        "\n",
        "    assert n > 0\n",
        "\n",
        "    s = (entropy_true * n_true + entropy_false * n_false) / n\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFI9qoMWSznZ"
      },
      "source": [
        "Độ lợi thông tin phân lớp tập D theo thuộc tính A:\n",
        "$$ Gain(A)=Entrophy(D)-Entrophy_{A}(D)$$\n",
        "\n",
        "Trong ID3, tại mỗi node, thuộc tính được chọn được xác định dựa trên là thuộc tính khiến cho information gain đạt giá trị lớn nhất.\n",
        "\n",
        "Các thuộc tính của tập Iris đều có giá trị liên tục. Do đó ta cần rời rạc hóa cho từng thuộc tính. Cách đơn giản là sử dụng một ngưỡng `cutoff` chia giá trị của dữ liệu trên mỗi thuộc tính sẽ làm 2 phần: `<cutoff` và `>=cutoff`.\n",
        "\n",
        "Để tìm ngưỡng `cutoff` tốt nhất cho mỗi thuộc tính ta lần lượt thay `cutoff` bằng các giá trị của thuộc tính sau đó tính entropy, `cutoff` tốt nhất khi entropy bé nhất $ \\left(\\arg\\min Entrophy_{A}(D)\\right)$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCQvubYDSzna"
      },
      "source": [
        "### 1.2 Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "npc0RldfSznd"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, tree=None, depth=0):\n",
        "        '''Parameters:\n",
        "        -----------------\n",
        "        tree: decision tree\n",
        "        depth: depth of decision tree after training'''\n",
        "        \n",
        "        self.depth = depth\n",
        "        self.tree=tree\n",
        "\n",
        "    def fit(self, X, y, node={}, depth=0):\n",
        "        '''Parameter:\n",
        "        -----------------\n",
        "        X: training data\n",
        "        y: label of training data\n",
        "        ------------------\n",
        "        return: node \n",
        "        \n",
        "        node: each node represented by cutoff value and column index, value and children.\n",
        "         - cutoff value is thresold where you divide your attribute\n",
        "         - column index is your data attribute index\n",
        "         - value of node is mean value of label indexes, \n",
        "           if a node is leaf all data samples will have same label\n",
        "        \n",
        "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
        "        '''\n",
        "        \n",
        "        # Stop conditions\n",
        "        \n",
        "        # if all value of y are the same \n",
        "        if np.all(y==y[0]):\n",
        "            return {'val':y[0]}\n",
        "\n",
        "        else: \n",
        "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
        "            y_left = y[X[:, col_idx] < cutoff]\n",
        "            y_right = y[X[:, col_idx] >= cutoff]\n",
        "            node = {'index_col':col_idx,\n",
        "                        'cutoff':cutoff,\n",
        "                   'val':np.mean(y)}\n",
        "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
        "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
        "            self.depth += 1 \n",
        "            self.tree = node\n",
        "            return node\n",
        "    \n",
        "    def find_best_split_of_all(self, X, y):\n",
        "        col_idx = None\n",
        "        min_entropy = 1\n",
        "        cutoff = None\n",
        "        for i, col_data in enumerate(X.T):\n",
        "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
        "            if entropy == 0: # best entropy\n",
        "                return i, cur_cutoff, entropy\n",
        "            elif entropy <= min_entropy:\n",
        "                min_entropy = entropy\n",
        "                col_idx = i\n",
        "                cutoff = cur_cutoff\n",
        "               \n",
        "        return col_idx, cutoff, min_entropy\n",
        "    \n",
        "    def find_best_split(self, col_data, y):\n",
        "        ''' Parameters:\n",
        "        -------------\n",
        "        col_data: data samples in column'''\n",
        "         \n",
        "        min_entropy = 10\n",
        "        cutoff = None\n",
        "\n",
        "        # Loop through col_data find cutoff where entropy is minimum\n",
        "        \n",
        "        for value in set(col_data):\n",
        "            y_predict = col_data < value\n",
        "            my_entropy = get_entropy(y_predict, y)\n",
        "\n",
        "            # min_entropy = cutoff = None\n",
        "            # TODO: calculate min_entropy, cutoff\n",
        "            # min_entropy = ?\n",
        "            # cutoff = ?\n",
        "\n",
        "            if my_entropy < min_entropy:\n",
        "                min_entropy = my_entropy\n",
        "                cutoff = value\n",
        "            \n",
        "        return min_entropy, cutoff\n",
        "                                               \n",
        "    def predict(self, X):\n",
        "        tree = self.tree\n",
        "        pred = np.zeros(shape=len(X))\n",
        "        for i, c in enumerate(X):\n",
        "            pred[i] = self._predict(c)\n",
        "        return pred\n",
        "    \n",
        "    def _predict(self, row):\n",
        "        cur_layer = self.tree\n",
        "        while cur_layer.get('cutoff'):\n",
        "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
        "                cur_layer = cur_layer['left']\n",
        "            else:\n",
        "                cur_layer = cur_layer['right']\n",
        "        else:\n",
        "            return cur_layer.get('val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3psFlX6gSznt"
      },
      "source": [
        "### 1.3 Classification on Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "i59zcSdUSznv",
        "outputId": "c9a8a376-f15b-4a75-8572-1ee95dfe99ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of your decision tree model on training data: 1.0\n",
            "Accuracy of your decision tree model: 0.8518518518518519\n"
          ]
        }
      ],
      "source": [
        "model = DecisionTreeClassifier()\n",
        "tree = model.fit(X_train, y_train)\n",
        "pred=model.predict(X_train)\n",
        "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train,pred))\n",
        "pred=model.predict(X_test)\n",
        "print('Accuracy of your decision tree model:', accuracy_score(y_test,pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: None - the scores for each class are returned\n",
            "Precision of your Gaussian Naive Bayes model: [1.         0.69230769 1.        ]\n",
            "Recall of your Gaussian Naive Bayes model: [1.         1.         0.57446809]\n",
            "F1 score of your Gaussian Naive Bayes model: [1.         0.81818182 0.72972973]\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: None - the scores for each class are returned')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average=None)\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives\n",
            "Precision of your Gaussian Naive Bayes model: 0.8518518518518519\n",
            "Recall of your Gaussian Naive Bayes model: 0.8518518518518519\n",
            "F1 score of your Gaussian Naive Bayes model: 0.8518518518518519\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='micro')\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
            "Precision of your Gaussian Naive Bayes model: 0.8974358974358975\n",
            "Recall of your Gaussian Naive Bayes model: 0.8581560283687942\n",
            "F1 score of your Gaussian Naive Bayes model: 0.8493038493038494\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='macro')\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjHLZYqNSzn6"
      },
      "source": [
        "## 2. Định lý Bayes\n",
        "\n",
        "Định lý Bayes được phát biểu dưới dạng toán học như sau:\n",
        "$$\\begin{equation}\n",
        "P\\left(A|B\\right)= \\dfrac{P\\left(B|A\\right)P\\left(A\\right)}{P\\left(B\\right)}\n",
        "\\end{equation}$$\n",
        "\n",
        "Nếu ta coi $B$ là dữ liệu $\\mathcal{D}$, các thông số cần ước tính $A$ là $w$, ta có:\n",
        "\n",
        "$$ \\begin{align}\n",
        "    \\underbrace{P(w|\\mathcal{D})}_{Posterior}= \\dfrac{1}{\\underbrace{P(\\mathcal{D})}_{Normalization}} \\overbrace{P(\\mathcal{D}|w)}^{\\text{Likelihood}} \\overbrace{P(w)}^{Prior}\n",
        "    \\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTTxdhtSzn8"
      },
      "source": [
        "#### Naive Bayes\n",
        "Để giúp cho việc tính toán được đơn giản, người ta thường giả sử một cách đơn giản nhất rằng các thành phần của biến ngẫu nhiên $D$ (hay các thuộc tính của dữ liệu $D$) là độc lập với nhau, nếu biết $w$. Tức là:\n",
        "$$P(\\mathcal{D}|w)=\\prod _{i=1}^{d}P(x_i|w)$$\n",
        "\n",
        "$d$: số lượng thuộc tính\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BqXEPzMSzn9"
      },
      "source": [
        "### 2.1. Probability Density Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "40JhBXHsSzn-"
      },
      "outputs": [],
      "source": [
        "class pdf:\n",
        "    def __init__(self,hist=None):\n",
        "        '''\n",
        "        A probability density function represented by a histogram\n",
        "        \n",
        "        hist: shape (n,1), n: number of hypotheses\n",
        "        hypo: hypothesis (simply understand as label)\n",
        "        ------------------\n",
        "        hist[hypo]=P(hypo)\n",
        "        '''\n",
        "        self.hist = hist\n",
        "        \n",
        "    # virtual function\n",
        "    def likelihood(self, data, hypo):\n",
        "        '''Paramters:\n",
        "        data: new data record \n",
        "        hypo: hypothesis (simply understand as label)\n",
        "        ---------\n",
        "        return P(data/hypo)\n",
        "        ''' \n",
        "        raise Exception()\n",
        "            \n",
        "    # update histogram for new data \n",
        "    def update(self, data):\n",
        "        ''' \n",
        "        P(hypo/data)=P(data/hypo)*P(hypo)*(1/P(data))\n",
        "        '''\n",
        "        \n",
        "        # Likelihood * Prior \n",
        "\n",
        "        # TODO: calculate self.hist\n",
        "        for hypo in self.hist.keys():\n",
        "            # self.hist[hypo] = ?\n",
        "            # self.hist[hypo] = None\n",
        "            self.hist[hypo] = self.hist[hypo] * self.likelihood(data, hypo)\n",
        "    \n",
        "        # Normalization\n",
        "        \n",
        "        # TODO: calculate s=P(data)\n",
        "        # s = ?\n",
        "        s = 0\n",
        "        for hypo in self.hist.keys():\n",
        "            s += self.hist[hypo]\n",
        "\n",
        "        for hypo in self.hist.keys():\n",
        "            self.hist[hypo] = self.hist[hypo] / s\n",
        "        \n",
        "    def plot_pdf(self):\n",
        "        # TODO: write a function to plot the histogram\n",
        "\n",
        "        list_keys = np.array(list(self.hist.keys()))\n",
        "        list_values = np.array(list(self.hist.values()))\n",
        "\n",
        "        plt.bar(list_keys, list_values, align='center')\n",
        "        plt.title('histogram')\n",
        "        plt.xlabel('hypothesis')\n",
        "        plt.ylabel('probability')\n",
        "        plt.show()\n",
        "\n",
        "        return None\n",
        "\n",
        "    def maxHypo(self):\n",
        "        # find the hypothesis (class) with maximum probability from hist\n",
        "        # and return its value\n",
        "        # TODO:\n",
        "\n",
        "        best_hypo = None\n",
        "        max_hist = -1\n",
        "\n",
        "        for hypo, _hist in self.hist.items():\n",
        "            if _hist > max_hist:\n",
        "                max_hist = _hist\n",
        "                best_hypo = hypo\n",
        "\n",
        "        return best_hypo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UImyM0ttSzoJ"
      },
      "source": [
        "### 2.2 Classification on Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lonlXQdWSzoL"
      },
      "source": [
        "#### Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD0ZmqplSzoM"
      },
      "source": [
        "- Naive Bayes có thể được mở rộng cho dữ liệu với các thuộc tính có giá trị là số thực, phổ biến nhất bằng cách sử dụng phân phối chuẩn (Gaussian distribution).\n",
        "\n",
        "- Phần mở rộng này được gọi là Gaussian Naive Bayes. Các hàm khác có thể được sử dụng để ước tính phân phối dữ liệu, nhưng Gauss (hoặc phân phối chuẩn) là dễ nhất để làm việc vì chỉ cần ước tính giá trị trung bình và độ lệch chuẩn từ dữ liệu huấn luyện."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax4E7_deSzoN"
      },
      "source": [
        "#### Define Gauss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOjpgllESzoO"
      },
      "source": [
        "$$ f\\left(x;\\mu,\\sigma \\right)= \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \n",
        "\\exp \\left({-\\dfrac{\\left(x-\\mu\\right)^2}{2 \\sigma^2}}\\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "iTHe4UlgSzoP"
      },
      "outputs": [],
      "source": [
        "def Gauss(std,mean,x):\n",
        "    # Calculate the Gaussian probability distribution function for x\n",
        "    # and return its value\n",
        "    # TODO: p = ?\n",
        "    exp = np.exp(-((x - mean)**2 / (2 * std**2)))\n",
        "    p = (1 / (std * np.sqrt(2 * np.pi))) * exp\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3989422804014327\n",
            "0.24197072451914337\n",
            "0.24197072451914337\n"
          ]
        }
      ],
      "source": [
        "# test Gauss\n",
        "\n",
        "print(Gauss(1.0, 1.0, 1.0))\n",
        "print(Gauss(1.0, 1.0, 2.0))\n",
        "print(Gauss(1.0, 1.0, 0.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "CmjU1ynKSzoW"
      },
      "outputs": [],
      "source": [
        "class NBGaussian(pdf):\n",
        "    def __init__(self, hist=None, std=None, mean=None):\n",
        "        '''Parameters:\n",
        "        \n",
        "        '''\n",
        "        pdf.__init__(self, hist)\n",
        "        self.std=std\n",
        "        self.mean=mean\n",
        "\n",
        "    def likelihood(self,data, hypo):\n",
        "        '''\n",
        "        Returns: res=P(data/hypo)\n",
        "        -----------------\n",
        "        Naive bayes:\n",
        "            Atributes are assumed to be conditionally independent given the class value.\n",
        "        '''\n",
        "    \n",
        "        std=self.std[hypo]\n",
        "        mean=self.mean[hypo]\n",
        "        res=1 \n",
        "\n",
        "        # TODO: compute the likelihood\n",
        "        # res=res*P(x1/hypo)*P(x2/hypo)...\n",
        "\n",
        "        for _std, _mean, _x in zip(std, mean, data):\n",
        "            _likelihood = Gauss(_std, _mean, _x)\n",
        "            res *= _likelihood\n",
        "\n",
        "        return res \n",
        "\n",
        "    def fit(self, X,y):\n",
        "        \"\"\"Parameters:\n",
        "        X: training data\n",
        "        y: labels of training data\n",
        "        \"\"\"\n",
        "        n=len(X)\n",
        "\n",
        "        # TODO: compute the number of iris species\n",
        "        # n_species=?\n",
        "        n_species = len(set(y))\n",
        "        \n",
        "        hist={}\n",
        "        mean={}\n",
        "        std={}\n",
        "        \n",
        "        #separate  dataset into rows by class\n",
        "        for hypo in range(0,n_species):\n",
        "            # rows have the hypothesis label\n",
        "            # TODO rows=?\n",
        "            # rows = None\n",
        "            rows = X[np.where(y == hypo)]\n",
        "\n",
        "            # histogram for each hypo\n",
        "            # TODO probability=?\n",
        "            # probability = None \n",
        "            # hist[hypo]=probability\n",
        "\n",
        "            propability = len(rows) / n\n",
        "            hist[hypo] = propability\n",
        "            \n",
        "            # Each hypothesis is represented by its mean and std \n",
        "            '''mean and standard deviation should be calculated for each column (or each attribute)'''\n",
        "            # TODO mean[hypo]=?, std[hypo]=?\n",
        "            # mean[hypo] = None\n",
        "            # std[hypo] = None\n",
        "\n",
        "            mean[hypo] = []\n",
        "            std[hypo] = []\n",
        "\n",
        "            for col in rows.T:\n",
        "                _mean = col.mean()\n",
        "                _std = col.std() \n",
        "\n",
        "                mean[hypo].append(_mean)\n",
        "                std[hypo].append(_std)\n",
        "         \n",
        "        self.mean=mean\n",
        "        self.std=std\n",
        "        self.hist=hist\n",
        "   \n",
        "    def _predict(self, data, plot=False):\n",
        "        \"\"\"\n",
        "        Predict label for only 1 data sample\n",
        "        ------------\n",
        "        Parameters:\n",
        "        data: data sample\n",
        "        plot: True: draw histogram after update new data sample\n",
        "        -----------\n",
        "        return: label of data\n",
        "        \"\"\"\n",
        "        model=NBGaussian(hist=self.hist.copy(),std=self.std.copy(), mean=self.mean.copy())\n",
        "        model.update(data)\n",
        "        if (plot): model.plot_pdf()\n",
        "        return model.maxHypo()\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\"Parameters:\n",
        "        Data: test data\n",
        "        ----------\n",
        "        return labels of test data\"\"\"\n",
        "        \n",
        "        pred=[]\n",
        "        for x in data:\n",
        "            pred.append(self._predict(x))\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eKUIQBqSzoZ"
      },
      "source": [
        "#### Show histogram of training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "yzxeLe-WSzoa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_1=NBGaussian()\n",
        "model_1.fit(X_train, y_train)\n",
        "model_1.plot_pdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY3riuZHSzoe"
      },
      "source": [
        "#### Test with 1 data record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "RY_sMQSYSzoe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label of X_test[10]:  0\n",
            "Our histogram after update X_test[10]: \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# label of y_test[10]\n",
        "print('Label of X_test[10]: ', y_test[10])\n",
        "# update model and show histogram with X_test[10]:\n",
        "\n",
        "print('Our histogram after update X_test[10]: ')\n",
        "model_1._predict(X_test[10],plot=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf7iK9bMSzoh"
      },
      "source": [
        "#### Evaluate your Gaussian Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JGPqkiRoSzok",
        "outputId": "50a529ce-5b2d-4a28-f25f-2a6ba3801d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of your Gaussian Naive Bayes model: 0.9481481481481482\n"
          ]
        }
      ],
      "source": [
        "pred=model_1.predict(X_test)\n",
        "print('Accuracy of your Gaussian Naive Bayes model:', accuracy_score(y_test,pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: None - the scores for each class are returned\n",
            "Precision of your Gaussian Naive Bayes model: [1.         0.88       0.97619048]\n",
            "Recall of your Gaussian Naive Bayes model: [1.         0.97777778 0.87234043]\n",
            "F1 score of your Gaussian Naive Bayes model: [1.         0.92631579 0.92134831]\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: None - the scores for each class are returned')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average=None)\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives\n",
            "Precision of your Gaussian Naive Bayes model: 0.9481481481481482\n",
            "Recall of your Gaussian Naive Bayes model: 0.9481481481481482\n",
            "F1 score of your Gaussian Naive Bayes model: 0.9481481481481482\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='micro')\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
            "Precision of your Gaussian Naive Bayes model: 0.9520634920634921\n",
            "Recall of your Gaussian Naive Bayes model: 0.9500394011032309\n",
            "F1 score of your Gaussian Naive Bayes model: 0.9492213680268086\n"
          ]
        }
      ],
      "source": [
        "print('Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account')\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='macro')\n",
        "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
        "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
        "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nhận xét nhanh Decision Tree vs Naive Bayes\n",
        "\n",
        "Decision Tree dự đoán bằng cách xây dựng 1 cấu trúc cây mà mỗi nút trong thể hiện 1 decision dựa vào giá trị còn nút lá đại diện cho label. Decision Tree dễ hiểu, dễ debug, nhưng nó cũng dễ bị overfitting (học tủ) nếu cái cây được phát triển quá lớn hoặc quá ít data.\n",
        "\n",
        "Naive Bayes là một mô hình xác suất dựa trên định lý Bayes. Cài đặt Naive Bayes nhanh, đơn giản, nhưng phương pháp này có thể hoạt động kém hiệu quả nếu giả thiết \"Các đặc trưng đưa vào mô hình là độc lập với nhau\" bị vi phạm.\n",
        "\n",
        "Trong ví dụ Iris này, với cách chia data:\n",
        "\n",
        "+ X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)\n",
        "\n",
        "Cả 2 model đều peform rất tốt, đều đạt được Accuracy 0.96. Các metric khác cũng khá tương đồng.\n",
        "\n",
        "+ X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.9, random_state=68)\n",
        "\n",
        "Lúc này, Naive Bayes đã thật sự outpeform Decision Tree. Decision Tree đạt được Accuracy 1.0 trên tập train nhưng chỉ được 0.85 trên tập test, đây chính là biểu hiện của overfitting. Ngược lại Gaussian Naive Bayes đạt được tới 0.95 Accuracy trên tập test. Các thông số còn lại, Decision Tree vẫn thấp hơn đáng kể Gaussian Naive Bayes."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hieunmt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7329050e1ba551351be0a542306158a45e0d9132df2d78f02ce7058e59c0ff58"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
