{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b76YngfGGfyD"
   },
   "source": [
    "# Lab03: Cây quyết định"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xZqh-Z7GfyF"
   },
   "source": [
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "- Làm trực tiếp vào notebook.\n",
    "- Làm những phần có chữ `TODO`.\n",
    "\n",
    "\n",
    "**Nội dung:**\n",
    "\n",
    "- Cây quyết định.\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--NRbml7GfyG"
   },
   "source": [
    "### Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhR1GCY5GfyH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm hỗ trợ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-OzYr2SGfyN"
   },
   "source": [
    "### Tải bộ dữ liệu Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oX5c3r4uGfyO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "#split dataset into training data and testing data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US1KgZBgGfyU"
   },
   "source": [
    "## 1. Cây quyết định: Iterative Dichotomiser 3 (ID3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sQh1ieuGfyV"
   },
   "source": [
    "### 1.1 Độ lợi thông tin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSjCJR_eGfyV"
   },
   "source": [
    "Giá trị kỳ vọng của self-information (entropy):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZM7fmb0GfyW"
   },
   "source": [
    "$$Entropy=-\\sum_{i}^{n}p_ilog_{2}(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDjtCHd_GfyX"
   },
   "source": [
    "Hàm entropy đạt giá trị nhỏ nhất nếu có giá trị $p_i$ bằng 1, đạt giá trị lớn nhất nếu tất cả $p_i$ đều bằng nhau. Những tính chất này của hàm entropy làm cho nó là một biểu hiện của sự mất trật tự, hay tính ngẫu nhiên của một hệ thống, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug variable\n",
    "\n",
    "# counts = [8]\n",
    "# n_samples = 8\n",
    "\n",
    "# col_data = [5.7, 7.6, 5.6, 5.1, 7.7, 5.8, 5.2, 5.0, 5.1, 5.0, 6.3, 4.8, 5.0, 5.1, 5.6, 5.1, 5.7, 7.7, 4.6, 6.2, 5.7, 5.5, 6.0, 5.8, 6.0, 5.4, 6.2, 5.5, 5.4, 5.0, 6.4, 5.0, 5.0, 5.5, 6.7, 4.9, 5.8, 5.0, 5.0, 5.9, 5.1, 6.9, 6.0, 6.1, 7.7, 5.5, 4.4, 4.3, 6.0, 7.2, 4.6, 5.1, 4.4, 6.3, 6.3, 4.6, 6.8, 6.3, 4.7, 6.1, 6.5, 6.2, 7.0, 6.4, 5.1, 6.9, 5.9, 6.5, 5.7, 5.2, 6.1, 4.5, 6.6, 5.5, 5.3, 5.6, 7.3, 6.7, 5.1, 4.9, 6.7, 7.2, 4.9, 6.7, 4.9, 6.9, 7.4, 6.3, 5.7, 6.5, 6.3, 6.4, 5.6, 5.9, 5.4, 6.1, 4.9, 5.8, 5.8, 7.1]\n",
    "# y = [1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2]\n",
    "\n",
    "# col_data = np.array(col_data)\n",
    "# y = np.array(y)\n",
    "\n",
    "# y_predict = [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, False, False, True, False, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
    "# y_predict = np.array(y_predict)\n",
    "\n",
    "# n = 100\n",
    "\n",
    "# division = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# division = np.array(division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kql-MFq-GfyX"
   },
   "outputs": [],
   "source": [
    "def entropy(counts, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts: shape (n_classes): list number of samples in each class\n",
    "    n_samples: number of data samples\n",
    "    \n",
    "    -----------\n",
    "    return entropy \n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    p_list = np.array(counts) / n_samples\n",
    "    en = 0\n",
    "\n",
    "    for p in p_list:\n",
    "        if p > 0:\n",
    "            en += p * np.log2(p)\n",
    "\n",
    "    return -en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AsGJfLhmGfyc"
   },
   "outputs": [],
   "source": [
    "def entropy_of_one_division(division): \n",
    "    \"\"\"\n",
    "    Returns entropy of a divided group of data\n",
    "    Data may have multiple classes\n",
    "    \"\"\"\n",
    "    n_samples = len(division)\n",
    "    n_classes = set(division)\n",
    "    \n",
    "    # counts = []\n",
    "    #count samples in each class then store it to list counts\n",
    "    #TODO:\n",
    "    \n",
    "    list_division = list(division)\n",
    "    counts = [list_division.count(c) for c in n_classes]\n",
    "    \n",
    "    return entropy(counts,n_samples), n_samples\n",
    "\n",
    "\n",
    "def get_entropy(y_predict, y):\n",
    "    \"\"\"\n",
    "    Returns entropy of a split\n",
    "    y_predict is the split decision by cutoff, True/Fasle\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "\n",
    "    entropy_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
    "    entropy_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
    "    \n",
    "    # overall entropy\n",
    "    #TODO s=?\n",
    "\n",
    "    assert n_true + n_false > 0\n",
    "\n",
    "    s = (entropy_true * n_true + entropy_false * n_false) / (n_true + n_false)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWhnKZm9Gfyi"
   },
   "source": [
    "Độ lợi thông tin của việc phân loại tập thông tin D theo thuộc tính A:\n",
    "$$ Gain(A)=Entrophy(D)-Entrophy_{A}(D)$$\n",
    "\n",
    "Tại mỗi nút trong ID3, một thuộc tính được chọn nếu mức tăng thông tin của nó cao nhất so với các nút khác.\n",
    "\n",
    "Tất cả các thuộc tính của bộ dữ liệu Iris được biểu diễn bằng các giá trị liên tục. Do đó chúng ta cần biểu diễn chúng bằng các giá trị rời rạc. Một cách đơn giản là sử dụng ngưỡng `cutoff` để tách các giá trị của dữ liệu trên mỗi thuộc tính thành hai phần: ` <cutoff` và `> = cutoff`.\n",
    "\n",
    "Để tìm `cutoff` tốt nhất cho một thuộc tính, chúng ta thay thế `cutoff` bằng các giá trị của nó rồi tính toán entropy, `cutoff` tốt nhất đạt được khi giá trị của entropy nhỏ nhất $ \\left (\\arg \\min Entrophy_ {A} (D) \\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTKwaSw-Gfyj"
   },
   "source": [
    "### 1.2 Cây quyết định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xp6omaz2Gfyj"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        min_entropy = log2(k), where k is the number of categories\n",
    "            => k = 3 => min_entropy = log2(3) ~ 1.58 \n",
    "            I just set it to 10 for simple\n",
    "    \"\"\"\n",
    "    def __init__(self, tree=None, depth=0):\n",
    "        '''Parameters:\n",
    "        -----------------\n",
    "        tree: decision tree\n",
    "        depth: depth of decision tree after training'''\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.tree=tree\n",
    "        \n",
    "    def fit(self, X, y, node={}, depth=0):\n",
    "        '''Parameter:\n",
    "        -----------------\n",
    "        X: training data\n",
    "        y: label of training data\n",
    "        ------------------\n",
    "        return: node \n",
    "        \n",
    "        node: each node represented by cutoff value and column index, value and children.\n",
    "         - cutoff value is thresold where you divide your attribute\n",
    "         - column index is your data attribute index\n",
    "         - value of node is mean value of label indexes, \n",
    "           if a node is leaf all data samples will have same label\n",
    "        \n",
    "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
    "        '''\n",
    "        \n",
    "        #Stop conditions\n",
    "    \n",
    "        # if y is empty\n",
    "        if not len(y):\n",
    "            return\n",
    "\n",
    "        #if all value of y are the same\n",
    "        if np.all(y==y[0]):\n",
    "            return {'val':y[0]}\n",
    "        else: \n",
    "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
    "\n",
    "            y_left = y[X[:, col_idx] < cutoff]\n",
    "            y_right = y[X[:, col_idx] >= cutoff]\n",
    "            node = {'index_col':col_idx,\n",
    "                        'cutoff':cutoff,\n",
    "                   'val':np.mean(y)}\n",
    "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
    "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
    "            self.depth += 1 \n",
    "            self.tree = node\n",
    "            return node\n",
    "    \n",
    "    def find_best_split_of_all(self, X, y):\n",
    "        col_idx = None\n",
    "        min_entropy = 10\n",
    "        cutoff = None\n",
    "\n",
    "        for i, col_data in enumerate(X.T):        \n",
    "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
    "\n",
    "            if entropy == 0:                   #best entropy\n",
    "                return i, cur_cutoff, entropy\n",
    "            elif entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                col_idx = i\n",
    "                cutoff = cur_cutoff\n",
    "               \n",
    "        return col_idx, cutoff, min_entropy\n",
    "    \n",
    "    def find_best_split(self, col_data, y):\n",
    "        ''' Parameters:\n",
    "        -------------\n",
    "        col_data: data samples in column'''\n",
    "         \n",
    "        min_entropy = 10\n",
    "        cutoff = None\n",
    "\n",
    "        #Loop through col_data find cutoff where entropy is minimum\n",
    "        \n",
    "        for value in set(col_data):\n",
    "            y_predict = col_data < value\n",
    "\n",
    "            my_entropy = get_entropy(y_predict, y)\n",
    "            #TODO\n",
    "            #min entropy=?, cutoff=?\n",
    "\n",
    "            if my_entropy <= min_entropy:\n",
    "                min_entropy = my_entropy\n",
    "                cutoff = value\n",
    "            \n",
    "        return min_entropy, cutoff\n",
    "                                               \n",
    "    def predict(self, X):\n",
    "        tree = self.tree\n",
    "        pred = np.zeros(shape=len(X))\n",
    "        for i, c in enumerate(X):\n",
    "            pred[i] = self._predict(c)\n",
    "        return pred\n",
    "    \n",
    "    def _predict(self, row):\n",
    "        cur_layer = self.tree\n",
    "        while cur_layer.get('cutoff'):\n",
    "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
    "                cur_layer = cur_layer['left']\n",
    "            else:\n",
    "                cur_layer = cur_layer['right']\n",
    "        else:\n",
    "            return cur_layer.get('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_OsIHd-Gfyq"
   },
   "source": [
    "### 1.3 Phân loại trên Bộ dữ liệu Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNgHip1dGfyr",
    "outputId": "12173b62-c713-4ad2-ca10-81d8addc7112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your decision tree model on training data: 1.0\n",
      "Accuracy of your decision tree model: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "tree = model.fit(X_train, y_train)\n",
    "pred=model.predict(X_train)\n",
    "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train,pred))\n",
    "pred=model.predict(X_test)\n",
    "print('Accuracy of your decision tree model:', accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rXS4sPCGfyz"
   },
   "source": [
    "## 2. Định lý Bayes\n",
    "\n",
    "Công thức Bayes\n",
    "$$\\begin{equation}\n",
    "P\\left(A|B\\right)= \\dfrac{P\\left(B|A\\right)P\\left(A\\right)}{P\\left(B\\right)}\n",
    "\\end{equation}$$\n",
    "\n",
    "Nếu $B$ là dữ liệu của chúng ta thì $\\mathcal{D}$, $A$ và $w$ là các tham số mà chúng ta cần ước tính:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    \\underbrace{P(w|\\mathcal{D})}_{Posterior}= \\dfrac{1}{\\underbrace{P(\\mathcal{D})}_{Normalization}} \\overbrace{P(\\mathcal{D}|w)}^{\\text{Likelihood}} \\overbrace{P(w)}^{Prior}\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuPXhYHzGfy1"
   },
   "source": [
    "#### Naive Bayes\n",
    "Để đơn giản hóa, người ta thường giả định rằng các thành phần của biến ngẫu nhiên $D$ (hoặc các features của dữ liệu $D$) là độc lập với nhau, nếu $w$ được biết. Nghĩa là:\n",
    "$$P(\\mathcal{D}|w)=\\prod _{i=1}^{d}P(x_i|w)$$\n",
    "\n",
    "- $d$: số lượng features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1m4AZLwgGfy3"
   },
   "source": [
    "### 2.1. Hàm mật độ xác suất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA3arZy8Gfy4"
   },
   "outputs": [],
   "source": [
    "class pdf:\n",
    "    def __init__(self,hist=None):\n",
    "        '''\n",
    "        A probability density function represented by a histogram\n",
    "        \n",
    "        hist: shape (n,1), n: number of hypotheses\n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        ------------------\n",
    "        hist[hypo]=P(hypo)\n",
    "        '''\n",
    "        self.hist = hist\n",
    "        \n",
    "    #virtual function\n",
    "    def likelihood(self, data, hypo):\n",
    "        '''Paramters:\n",
    "        data: new data record \n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        ---------\n",
    "        return P(data/hypo)\n",
    "        ''' \n",
    "        raise Exception()\n",
    "            \n",
    "    #update histogram for new data \n",
    "    def update(self, data):\n",
    "        ''' \n",
    "        P(hypo/data)=P(data/hypo)*P(hypo)*(1/P(data))\n",
    "        '''\n",
    "        \n",
    "        #Likelihood * Prior \n",
    "        #TODO\n",
    "        \n",
    "        for hypo in self.hist.keys():\n",
    "            #self.hist[hypo]=?\n",
    "            \n",
    "            self.hist[hypo] = self.hist[hypo] * self.likelihood(data, hypo)\n",
    "            \n",
    "        #Normalization\n",
    "        \n",
    "        #TODO: s=P(data)\n",
    "        #s=?\n",
    "        \n",
    "        s = 1\n",
    "        \n",
    "        for hypo in self.hist.keys():\n",
    "            self.hist[hypo] = self.hist[hypo] / s\n",
    "        \n",
    "    def plot_pdf(self):\n",
    "        #plot Histogram\n",
    "        #TODO\n",
    "\n",
    "        list_keys = np.array(list(self.hist.keys()))\n",
    "        list_values = np.array(list(self.hist.values()))\n",
    "\n",
    "        plt.bar(list_keys, list_values, align='center')\n",
    "        plt.title('histogram')\n",
    "        plt.show()\n",
    "    \n",
    "    def maxHypo(self):\n",
    "        #find the hypothesis with maximum probability from hist\n",
    "        #TODO\n",
    "\n",
    "        best_hypo = None\n",
    "        max_hist = -1\n",
    "\n",
    "        for hypo, _hist in self.hist.items():\n",
    "            if _hist > max_hist:\n",
    "                max_hist = _hist\n",
    "                best_hypo = hypo\n",
    "\n",
    "        return best_hypo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x89nTrIEGfy7"
   },
   "source": [
    "### 2.2 Phân loại trên Bộ dữ liệu Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9moO4N2Gfy8"
   },
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhLm2pD-Gfy-"
   },
   "source": [
    "- Naive Bayes có thể được mở rộng để sử dụng trên dữ liệu liên tục, phổ biến nhất là bằng cách sử dụng phân phối chuẩn (phân phối Gaussian).\n",
    "\n",
    "- Phần mở rộng này được gọi là Gaussian Naive Bayes. Các hàm khác có thể được sử dụng để ước tính phân phối dữ liệu, nhưng Gauss (hoặc phân phối chuẩn) là dễ làm việc nhất vì chúng ta chỉ cần ước tính giá trị trung bình và độ lệch chuẩn từ dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDbWOYQ-GfzA"
   },
   "source": [
    "#### Định nghĩa hàm Gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmlbwFHPGfzA"
   },
   "source": [
    "$$ f\\left(x;\\mu,\\sigma \\right)= \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "\\exp \\left({-\\dfrac{\\left(x-\\mu\\right)^2}{2 \\sigma^2}}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OpY89tkGfzB"
   },
   "outputs": [],
   "source": [
    "def Gauss(std,mean,x):\n",
    "    #Compute the Gaussian probability distribution function for x\n",
    "    #TODO \n",
    "    exp = np.exp(-((x - mean)**2 / (2 * std**2)))\n",
    "    prob = (1 / (std * np.sqrt(2 * np.pi))) * exp\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3989422804014327\n",
      "0.24197072451914337\n",
      "0.24197072451914337\n"
     ]
    }
   ],
   "source": [
    "# test Gauss\n",
    "\n",
    "print(Gauss(1.0, 1.0, 1.0))\n",
    "print(Gauss(1.0, 1.0, 2.0))\n",
    "print(Gauss(1.0, 1.0, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIutnepWGfzH"
   },
   "outputs": [],
   "source": [
    "class NBGaussian(pdf):\n",
    "    def __init__(self, hist=None, std=None, mean=None):\n",
    "        '''Parameters:\n",
    "        \n",
    "        '''\n",
    "        pdf.__init__(self, hist)\n",
    "        self.std=std\n",
    "        self.mean=mean\n",
    "        \n",
    "    def likelihood(self, data, hypo):\n",
    "        '''\n",
    "        Returns: res=P(data/hypo)\n",
    "        -----------------\n",
    "        Naive bayes:\n",
    "            Atributes are assumed to be conditionally independent given the class value.\n",
    "        '''\n",
    "    \n",
    "        std=self.std[hypo]\n",
    "        mean=self.mean[hypo]\n",
    "        res=1\n",
    "\n",
    "        #TODO\n",
    "        #res=res*P(x1/hypo)*P(x2/hypo)...\n",
    "\n",
    "        for _std, _mean, _x in zip(std, mean, data):\n",
    "            _likelihood = Gauss(_std, _mean, _x)\n",
    "            res *= _likelihood\n",
    "            \n",
    "        return res \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Parameters:\n",
    "        X: training data\n",
    "        y: labels of training data\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        #number of iris species\n",
    "        #TODO\n",
    "        #n_species=???\n",
    "\n",
    "        n_species = len(set(y))\n",
    "\n",
    "        hist={}\n",
    "        mean={}\n",
    "        std={}\n",
    "        \n",
    "        #separate dataset into rows by class\n",
    "        for hypo in range(0,n_species):\n",
    "            #rows have hypo label\n",
    "            #TODO rows=\n",
    "        \n",
    "            #histogram for each hypo\n",
    "            #TODO probability=?\n",
    "            \n",
    "            # hist[hypo]=propability\n",
    "            \n",
    "            #Each hypothesis represented by its mean and standard derivation\n",
    "            '''mean and standard derivation should be calculated for each column (or each attribute)'''\n",
    "            #TODO mean[hypo]=?, std[hypo]=?\n",
    "\n",
    "            rows = X[np.where(y == hypo)]\n",
    "        \n",
    "            propability = len(rows) / n\n",
    "            hist[hypo] = propability\n",
    "            mean[hypo] = []\n",
    "            std[hypo] = []\n",
    "\n",
    "            for col in rows.T:\n",
    "                _mean = col.mean()\n",
    "                _std = col.std() \n",
    "\n",
    "                mean[hypo].append(_mean)\n",
    "                std[hypo].append(_std)\n",
    "         \n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    "        self.hist=hist\n",
    "   \n",
    "    def _predict(self, data, plot=False):\n",
    "        \"\"\"\n",
    "        Predict label for only 1 data sample\n",
    "        ------------\n",
    "        Parameters:\n",
    "        data: data sample\n",
    "        plot: True: draw histogram after update new record\n",
    "        -----------\n",
    "        return: label of data\n",
    "        \"\"\"\n",
    "        model=NBGaussian(hist=self.hist.copy(),std=self.std.copy(), mean=self.mean.copy())\n",
    "        model.update(data)\n",
    "        if (plot): model.plot_pdf()\n",
    "        return model.maxHypo()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Parameters:\n",
    "        Data: test data\n",
    "        ----------\n",
    "        return labels of test data\"\"\"\n",
    "        pred=[]\n",
    "        for x in data:\n",
    "            pred.append(self._predict(x))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Gmv2qqxGfzM"
   },
   "source": [
    "#### Hiển thị histogram của dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkjhuGkLGfzN",
    "outputId": "0cacea13-c482-4706-f759-2da97552fe4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1=NBGaussian()\n",
    "model_1.fit(X_train, y_train)\n",
    "model_1.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itiPerhxGfzW"
   },
   "source": [
    "#### Kiểm tra với 1 data record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ev9UTlb6GfzX",
    "outputId": "ad1d31ca-6cf9-4081-a0fe-ff7c65963db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of X_test[10]:  2\n",
      "Our histogram after update X_test[10]: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label of y_test[10]\n",
    "print('Label of X_test[10]: ', y_test[10])\n",
    "#update model and show histogram with X_test[10]:\n",
    "\n",
    "print('Our histogram after update X_test[10]: ')\n",
    "model_1._predict(X_test[10],plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAhcGUTgGfzd"
   },
   "source": [
    "#### Đánh giá mô hình Gaussian Naive Bayes của bạn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5Kvk-rUGfze",
    "outputId": "9ba3c648-275d-446f-d3f2-b0256a07a241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your Gaussian Naive Bayes model: 0.96\n"
     ]
    }
   ],
   "source": [
    "pred=model_1.predict(X_test)\n",
    "print('Accuracy of your Gaussian Naive Bayes model:', accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJaWYqt5Jvmp"
   },
   "source": [
    "**TODO**: Báo cáo về F1, Recall và Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mode: None - the scores for each class are returned\n",
      "\n",
      "[19 15 16]\n",
      "Precision of your Gaussian Naive Bayes model: [1.         0.93333333 0.9375    ]\n",
      "Recall of your Gaussian Naive Bayes model: [1.         0.93333333 0.9375    ]\n",
      "F1 score of your Gaussian Naive Bayes model: [1.         0.93333333 0.9375    ]\n"
     ]
    }
   ],
   "source": [
    "print('Average mode: None - the scores for each class are returned')\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average=None)\n",
    "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
    "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
    "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives\n",
      "Precision of your Gaussian Naive Bayes model: 0.96\n",
      "Recall of your Gaussian Naive Bayes model: 0.96\n",
      "F1 score of your Gaussian Naive Bayes model: 0.96\n"
     ]
    }
   ],
   "source": [
    "print('Average mode: micro - calculate metrics globally by counting the total true positives, false negatives and false positives')\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='micro')\n",
    "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
    "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
    "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
      "Precision of your Gaussian Naive Bayes model: 0.9569444444444445\n",
      "Recall of your Gaussian Naive Bayes model: 0.9569444444444445\n",
      "F1 score of your Gaussian Naive Bayes model: 0.9569444444444445\n"
     ]
    }
   ],
   "source": [
    "print('Average mode: macro - calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account')\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "print('Precision of your Gaussian Naive Bayes model:', precision)\n",
    "print('Recall of your Gaussian Naive Bayes model:', recall)\n",
    "print('F1 score of your Gaussian Naive Bayes model:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab04-DecisionTree&BayesTheorem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hieunmt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7329050e1ba551351be0a542306158a45e0d9132df2d78f02ce7058e59c0ff58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
